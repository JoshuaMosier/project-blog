// Generated file - do not edit directly
export const postContent = {
  "cinetrics": "View project on Devpost → Ever spend more time scrolling through Netflix than actually watching something? We built Cinetrics to solve this common dilemma by matching you with film critics who share your taste in movies. ## The Problem With countless streaming options available today, finding quality content has become increasingly challenging. While there are many movie critics out there, finding one whose taste aligns with yours traditionally required reading through countless reviews - until now. ## Our Solution Cinetrics is an intelligent recommendation platform that pairs users with professional movie critics based on their viewing preferences. By rating just a handful of movies, our algorithm identifies critics whose taste matches yours, providing personalized and reliable movie recommendations. ## Technical Implementation We built Cinetrics using a modern tech stack focused on scalability and real-time processing: - **Frontend**: React-based single-page application - **Backend**: Python API with asynchronous event handling - **Database**: CockroachDB for reliable data storage - **Infrastructure**: Google Cloud Platform (App Engine and Cloud SQL) - **Data Sources**: Rotten Tomatoes critic reviews and TMDb API for movie metadata ### Smart Rating Analysis Unlike traditional review aggregators (IMDb, Rotten Tomatoes, Metacritic), we developed a sophisticated algorithm that: - Normalizes different rating scales used by critics - Aligns critic opinions with audience perspectives - Requires minimal user input to generate accurate matches - Uses encrypted communication and secure password storage ## Future Developments We're working on implementing machine learning capabilities to: - Predict critic ratings for unreviewed movies - Generate realistic movie ratings based on review history - Reduce the number of user ratings needed for accurate matching *This project won the MLH Best Use of CockroachDB award at VTHacks 8.*",
  "maglev-baby-yoda": "A custom magnetic levitating Baby Yoda display I created as a Christmas gift for my sister, combining 3D printing, painting, and electronics to bring a piece of The Mandalorian to life. ## The Build Process This project was a labor of love that took about 3-4 weeks to complete. It combines several elements: - 3D printed Baby Yoda model from Inspyre 3D - Modified floating pram from Multiverse3DDesigns - Custom-designed base created in Meshmixer - Magnetic levitation kit for the floating effect - Hand-painted details using acrylics and wash paints ## Technical Details The build involved approximately 15 hours of total print time: - 10 hours for the pram - 2.5 hours for Baby Yoda - 2.5 hours for the base The base features a Beskar ingot-inspired design with the Empire logo, cleverly concealing the magnetic levitation electronics. The finished piece measures about 15 centimeters in diameter, making it a perfect desk display piece. ## Learning Experience This project represented several firsts for me, including my first experience with finishing and painting 3D prints. While there was definitely a learning curve, the end result was worth the effort. The combination of the floating pram and the hand-painted details really brings the piece to life. [View an article about the project →](https://htxt.co.za/2020/01/3d-printed-baby-yoda-has-a-magnetic-levitation-pram/)",
  "movie-posters": "View full resolution mosaic→ I built a Python application that generates movie posters made up of thousands of tiny movie poster thumbnails. The program analyzes colors to create a larger image mosaic while maintaining visual coherence. ## The Technical Challenge Creating photo mosaics presents several interesting technical challenges: - Analyzing and matching colors effectively across thousands of images - Optimizing image processing for reasonable performance - Maintaining visual quality at both macro and micro scales - Handling different poster aspect ratios and sizes The core of the solution uses k-d trees for efficient color matching and PIL for image processing. Here's a simplified example of how the color matching works: ## Image Processing Pipeline The full process involves several steps: 1. Download and preprocess movie poster images 2. Extract dominant colors from each poster 3. Build a searchable color database 4. Generate the mosaic by matching target colors 5. Create final output in multiple sizes with proper DPI ## Optimization Techniques Some key optimizations I implemented: - Caching preprocessed image data in JSON - Using numpy for faster color calculations - Implementing parallel processing for image analysis - Pre-filtering images based on color characteristics ## Production Features Beyond the core mosaic generation, I added several features for production use: - Multiple output sizes (16x20\", 18x24\", 24x36\") - Border and borderless variants - High-resolution 300 DPI output - Preview image generation - Automated listing photo creation ## Lessons Learned This project taught me a lot about: - Color space mathematics and perception - Image processing optimization techniques - Working with large datasets of images - Balancing quality vs performance tradeoffs The most challenging aspect was tuning the color matching algorithm to produce visually pleasing results while maintaining reasonable processing times. *Built with Python, PIL, numpy, and lots of movie posters*",
  "nutricient": "View project on Devpost → A web application that optimizes meal choices in Virginia Tech dining halls using linear programming to help students maintain balanced nutrition while meeting their dietary goals. ## The Problem Many college students struggle to maintain a balanced diet while eating in dining halls. Whether trying to meet specific fitness goals, manage allergies, or simply eat healthily, it can be overwhelming to analyze nutritional information for every meal option. My brother's quest to maximize protein intake while maintaining balanced nutrition inspired me to tackle this challenge at scale. ## The Solution Nutricient is a web application that: - Automatically scrapes nutritional data from VT dining halls - Takes user inputs for: - Metabolic information - Dietary restrictions and allergies - Preferred dining locations - Target nutrient goals - Uses linear optimization (PuLP) to calculate ideal meal combinations - Minimizes calories while meeting nutritional targets - Allows users to iteratively refine results by excluding unwanted items ## Technical Implementation The application stack includes: - Python/Flask backend deployed on Google App Engine - Bootstrap-based single-page application frontend - Automated data collection using Google Cloud Functions and Cloud Scheduler - Data storage in Google Cloud Storage buckets - PuLP library for linear optimization calculations ## Key Challenges The biggest hurdles came from working with Google Cloud Platform: - Setting up reliable cron jobs for data collection - Managing cloud function deployments - Configuring storage buckets efficiently - Domain routing and DNS configuration ## Future Developments The project has several planned expansions: - Including on-campus restaurant nutritional data - Adding local grocery store options - Incorporating complete meal recipes - Improving edge case handling and form validation - Enhanced UI/UX improvements This project demonstrates how mathematical optimization can be applied to everyday problems, making it easier for students to maintain healthy eating habits while navigating campus dining options. [View the project on Devpost →](https://devpost.com/software/nutricient) *This project won 3rd place at VTHacks 7.*",
  "on-air": "View project on Kickstarter → It started with a simple problem: interruptions. Like many people, I’ve struggled to stay productive while working from home. Whether it was a roommate popping in during a meeting or family members not realizing I was trying to concentrate, I found myself needing a better way to communicate “I’m busy.” ## The Idea The concept was pretty straightforward: a small light that anyone could use to show when they’re busy, available, or need some quiet time. But I wanted to take it a step further. Instead of just being a “red light/green light” system, I thought: - Why not make it smart? - Why not integrate it with tools people already use, like Teams, Zoom, or Slack? ## What On Air Does *On Air* is essentially a connected status indicator, but it’s packed with features to make it flexible and useful: - **Customizable Colors and Patterns:** You can set up the colors and animations that work for you—whether it’s a steady red light for “Do Not Disturb”, green for \"I'm available”, or yellow for \"My meeting ends soon.” - **App and Calendar Syncing:** The light can automatically update based on your meeting schedule or even your status in tools like Microsoft Teams. - **Voice Assistant Integration:** You can change your status hands-free with Alexa or Google Assistant. - **Smart Automations:** The light can adapt to your environment. For example, it dims in the evening or changes based on triggers you set. ## Building the Project I learned a lot while building this. The biggest challenge was getting it to integrate seamlessly with all the tools people use—everyone has their own workflow, and *On Air* needed to fit into as many as possible. We also had to make sure it was easy to use right out of the box while still offering advanced features for power users. The hardware itself went through several iterations to get the size, brightness, and design just right. I wanted it to look clean and modern—something that could sit on a desk without feeling out of place. ## What’s Next There are so many possibilities for where this project could go. Some ideas I’m excited about: - Adding more platform integrations for things like Zoom Room systems or gaming setups. - Expanding environmental awareness, like automatically adjusting brightness based on room lighting. - Creating specialized modes for different use cases, like “focus” sessions or parental controls. ## Why It Matters Ultimately, I hope *On Air* makes it a little easier to carve out focus time in an increasingly noisy world. Whether you’re working, studying, or streaming, it’s a simple tool to help others respect your time and space.",
  "plotify": "View project on Devpost → Ever wondered what your music taste really looks like? While Spotify Wrapped gives us a yearly glimpse into our listening habits, I wanted to create something that could provide deeper insights at any time. Enter Plotify: a web application that transforms your Spotify data into meaningful visualizations. ## What is Plotify? Plotify is a web application that connects to your Spotify account to analyze and visualize your listening habits. It displays detailed information about your top artists, tracks, genres, and playlists through interactive charts and graphs. ## Technical Implementation Built during HooHacks 2021 in just 24 hours, Plotify combines several technologies: - **Backend**: Flask (Python) handles the server-side logic and Spotify API authentication - **Data Visualization**: Combination of D3.js and Chart.js for creating interactive visualizations - **API Integration**: Direct integration with Spotify's API for fetching user data - **Data Processing**: Python scripts for processing and organizing Spotify data into meaningful visualizations ## Features - Authorization with Spotify accounts - Visualization of top artists and tracks - Genre distribution analysis - Playlist insights - Timeline-based listening patterns ## Challenges and Learnings Building Plotify in 24 hours came with its share of challenges. Working with charting libraries under time pressure required quick decision-making and efficient troubleshooting. The project taught me valuable lessons about API integration, data visualization, and rapid development. ## Future Plans The current version of Plotify is just the beginning. Future plans include: - Support for uploading complete listening history - Long-term trend analysis - Music preference evolution tracking - Enhanced visualization options *This project was built for HooHacks 2021.*",
  "pullback": "Watch the demonstration → For our kinematics term project, we developed a functional pullback toy car mechanism using 3D printing. The mechanism uses a gear system and spring to convert backward motion into forward movement through a dual gear train system. ## How It Works The pullback mechanism operates through a simple principle: when the car is pulled backward, the rear wheels' rotation engages a gear train connected to a torsion spring. The gearbox contains two gear trains - one for winding (reverse) and one for release (forward). Our calculations showed: - Theoretical output/input ratio: 12.25:1 - Measured displacement ratio: 22-25:1 - Reverse gear train ratio: 147:8 - Forward gear train ratio: 1.5:1 ## Design and Implementation The project involved: - CAD modeling in Fusion 360 - Gear design optimization to reduce interference - 3D printing on an Artillery Sidewinder X1 - Assembly with focus on the spring mechanism - TPU material for rear tire traction The design uses translating pink and orange gears that mesh with the main blue gear. This creates separate gear trains for forward and reverse movement, allowing for the difference between input and output displacement. ## Testing Results Testing of the mechanism showed: - Travel distance up to 15 feet from full pullback - First prototype: 22:1 displacement ratio - Second prototype: 25:1 displacement ratio - Consistent performance in repeated tests ## Lessons Learned The project provided experience with: - Gear train design - 3D printing tolerances - Spring mechanics - Material selection [Watch the demonstration →](https://www.youtube.com/watch?v=a9JcV-hAEtk) *This project was completed as part of ME 3604 coursework by Josh Mosier and Zach Sucher.*",
  "senior-design": "import ObjViewer from '$lib/components/ObjViewer.svelte'; View paper on Arxiv → As part of my senior design team at Virginia Tech, we tackled this problem by developing a novel haptic wristband that could bridge the communication gap between robots and their human operators. ## The Problem When working with robotic systems, particularly in shared autonomy scenarios, it's often unclear what the robot has learned or when it needs additional guidance. Traditional visual feedback requires constantly watching the robot, which isn't always practical when you need to multitask. We needed a way to let the robot \"tap you on the wrist\" when it needed help. ## The Solution Our solution was a lightweight, wireless haptic wristband that provides two types of feedback: - A gentle squeeze mechanism that can alert the user when the robot needs attention - Six vibrotactors arranged around the wrist that can create directional patterns to guide user input ## Building the Device ### The Squeeze Mechanism One of our biggest innovations was the cord and reel system controlled by a high-torque N20 DC motor. We designed it to provide gentle but noticeable squeeze notifications while keeping the band comfortable during extended wear. The cord is never in direct contact with the wearer's skin, and we normally keep it in a relaxed state. ### Vibrotactile Feedback We incorporated six cylindrical ERM motors evenly spaced around the wristband. Each vibrotactor sits in a custom resin-printed housing that transfers vibrations directly to the skin. One of the trickiest parts was designing the accordion-style band that could fit different wrist sizes while keeping vibrations isolated between motors. ### Housing Design The electronics housing went through several iterations before we got it right. We needed to fit the microcontroller, battery, squeeze mechanism, and motor drivers in a compact package while keeping it lightweight and comfortable. The final design uses a modular approach that makes assembly and maintenance straightforward. ## Technical Challenges Some of the biggest challenges we faced included: - Designing a flexible band that could accommodate different wrist sizes while maintaining consistent haptic feedback - Creating a squeeze mechanism that was both gentle and reliable - Managing power consumption to ensure the wireless device could operate for extended periods - Isolating vibrations between motors to create clear directional cues ## Results Our user studies showed that the wristband, when combined with augmented reality feedback, significantly improved human-robot interaction. Users could focus on other tasks while remaining responsive to the robot's needs, and the directional haptic cues helped them provide more effective guidance when needed. ## Future Possibilities This project opened up exciting possibilities for human-robot interaction. Some potential future developments include: - Adding more sophisticated haptic patterns for complex communications - Incorporating additional sensors for context-aware feedback - Developing specialized versions for different industrial or assistive robotics applications - Exploring applications beyond robotics in areas like VR/AR interaction --- Looking back, this project taught me invaluable lessons about hardware development, user-centered design, and the importance of multimodal feedback in human-robot interaction. It's exciting to see how a relatively simple device can make such a meaningful difference in how humans and robots work together.",
  "speakirby": "For our family's \"Sibling Santa\" gift exchange, I designed and 3D-printed a Super Smash Bros. Melee-styled trophy featuring Kirby, with a removable Bluetooth speaker module integrated into his mouth. ## Design Process The design was inspired by the trophy collection from Super Smash Bros. Melee, featuring Kirby in his classic pose on a golden base. I modeled the design in Fusion 360, ensuring the mouth opening would perfectly fit a standard Bluetooth speaker module while maintaining the character's iconic proportions. - Created the basic Kirby shape using reference images from SSBM - Designed the mouth cavity to securely hold a removable Bluetooth speaker - Engineered the base with proper support for the top-heavy design - Added subtle details to match the SSBM trophy aesthetic ## Printing and Finishing The trophy was printed in multiple parts using PLA: - Main Kirby body in pink PLA - Base in gold-colored PLA - Custom-designed internal mounting system for the speaker The finishing process was crucial for achieving a professional look: 1. Carefully primed the printed parts 2. Airbrushed the body to achieve a smooth, even pink finish 3. Created custom eye designs and printed them on sticker paper 4. Applied the eye stickers 5. Sealed with clear coat for durability",
  "streamlit-runner": "View on VS Code Marketplace → I created a small VS Code extension to solve a minor annoyance in my Streamlit development workflow. Instead of typing `streamlit run` in the terminal each time, I wanted a simple right-click option to launch my apps. ## Why I Built This While working on various Streamlit projects, I found myself repeatedly typing the same terminal command. The solution was straightforward - add a context menu option to do this automatically. Nothing groundbreaking, just a small quality-of-life improvement. ## The Implementation The extension is quite simple, consisting of just a few core components: - A command registration for the context menu - Basic terminal management for running the Streamlit command - Simple error handling for missing dependencies ## What I Learned This was my first time creating a VS Code extension, and while it's a very basic one, I learned about: - The basic structure of VS Code extensions - How to interact with VS Code's terminal API - The process of publishing to the VS Code marketplace ## Next Steps While the extension is intentionally minimal, I might add a few small features if users request them: - Basic configuration options - Support for common Streamlit CLI arguments - Simple status indicators",
  "vt-map": "A handcrafted 3D wooden map of Virginia Tech's campus, created as a unique Secret Santa gift. This multi-layered piece combines laser-cut Baltic birch plywood with detailed engravings of campus roads, paths, and buildings to create a striking dimensional representation of the university grounds. ## Design Process The project began with a Virginia Tech campus PDF map, which I converted into separate vector layers for the various map elements. This allowed for precise control over: - Building footprints for the raised 3D elements - Road networks and walking paths - Background terrain details - Campus boundaries and landmarks ## Fabrication Using the makerspace laser cutter in my dorm, I brought the design to life through multiple steps: - Base layer: Laser engraved the ground details and cut the road networks - Building layer: Precision cut all campus buildings from a second sheet of Baltic birch - Assembly: Carefully glued each building in its exact location - Finishing touches: Custom routed wooden frame with glass cover - Final detail: Etched Virginia Tech logo on the glass ## Technical Details The map was created using: - Baltic birch plywood for durability and clean cutting - Multiple laser cutter passes for varying depths - Vector and raster operations for different map elements - Custom frame routing for professional finish - Glass overlay for protection and added dimension *Created in 2020 as part of a Secret Santa exchange*"
};
